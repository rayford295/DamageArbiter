# ğŸŒªï¸ A CLIP-Enhanced Multimodal Arbitration Framework for Explainable Disaster Damage Assessment from Street-View Imagery

This repository contains the implementation, figures, and dataset links for the paper:  
**â€œA CLIP-Enhanced Multimodal Arbitration Framework for Explainable Hurricane-Induced Damage Assessment from Street-View Imagery.â€**

---

## ğŸ“˜ Overview

This study proposes a **CLIP-enhanced multimodal arbitration framework** designed to improve the interpretability, reliability, and accuracy of street-view-based disaster damage assessment.  
It systematically combines **Vision Transformer (ViT)** and **CLIP (Contrastive Languageâ€“Image Pretraining)** representations, supported by **LLM-generated disaster annotations**.

---

## ğŸ§© Methodology Framework

<p align="center">
  <img src="figure/figure3.Methodology framework.png" alt="Methodology Framework" width="700">
</p>

The framework integrates:
- Vision-based feature extraction (ViT)
- LLM-assisted textual annotation generation
- CLIP-based cross-modal alignment
- Confidence-based arbitration for explainable disaster damage prediction

---

## ğŸ“Š Figures

### **Figure 1. Study Area**
<p align="center">
  <img src="figure/figure1. studyarea map.png" alt="Study Area Map" width="600">
</p>

### **Figure 2. Label Example**
<p align="center">
  <img src="figure/figure2.Label-example.png" alt="Label Example" width="600">
</p>

### **Figure 4. Vision Transformer Architecture**
<p align="center">
  <img src="figure/figure4. VIT.png" alt="ViT Architecture" width="600">
</p>

### **Figure 5. CLIP Model**
<p align="center">
  <img src="figure/figure5. clip.png" alt="CLIP Model" width="600">
</p>

### **Figure 6. Framework of Confidence-Based Error Analysis**
<p align="center">
  <img src="figure/figure6. Framework of Confidence-Based Error Analysis.png" alt="Confidence-Based Error Analysis" width="600">
</p>

### **Figure 7. Semantic Detection Pipeline**
<p align="center">
  <img src="figure/figure7. Semantic Detection.png" alt="Semantic Detection" width="600">
</p>

### **Figure 8. Spatial Mapping Results**
<p align="center">
  <img src="figure/figure8.mapping.png" alt="Mapping Results" width="600">
</p>

---

## ğŸ“‚ Dataset

You can access the **street-view disaster dataset** from the following DOI:

> **Yang, Yifan (2025)**  
> *Perceiving Multidimensional Disaster Damages from Streetâ€“View Images Using Visualâ€“Language Models*  
> [ğŸ“ figshare Dataset DOI: 10.6084/m9.figshare.28801208.v2](https://doi.org/10.6084/m9.figshare.28801208.v2)

The dataset includes:
- Pre- and post-disaster street-view imagery  
- Georeferenced location and damage type annotations  
- Severity levels (*mild*, *moderate*, *severe*)  
- Sample image regions from **Horseshoe Beach, Florida**, after **Hurricane Milton**

---
---

## ğŸ§­ Repository Structure

ğŸ“¦ **CLIP-Enhanced-4hurricane**  
â”‚  
â”œâ”€â”€ ğŸ“ **code/** â€” Source code for model training and evaluation  
â”‚   â”œâ”€â”€ ğŸ§  `inference.py` â€” Inference and prediction pipeline  
â”‚   â”œâ”€â”€ âš™ï¸ `train_clip.py` â€” CLIP model fine-tuning and multimodal arbitration  
â”‚   â””â”€â”€ ğŸ§© `utils/` â€” Utility functions and helper scripts  
â”‚  
â”œâ”€â”€ ğŸ“ **dataset/** â€” Dataset structure and metadata  
â”‚   â”œâ”€â”€ ğŸ—‚ï¸ `metadata.csv` â€” Metadata and label information  
â”‚   â””â”€â”€ ğŸŒ `samples/` â€” Sample image pairs and annotations  
â”‚  
â”œâ”€â”€ ğŸ–¼ï¸ **figure/** â€” Figures used in the paper and documentation  
â”‚   â”œâ”€â”€ `figure1.studyarea map.png`  
â”‚   â”œâ”€â”€ `figure2.Label-example.png`  
â”‚   â”œâ”€â”€ `figure3.Methodology framework.png`  
â”‚   â”œâ”€â”€ `figure4.VIT.png`  
â”‚   â”œâ”€â”€ `figure5.clip.png`  
â”‚   â”œâ”€â”€ `figure6.Framework of Confidence-Based Error Analysis.png`  
â”‚   â”œâ”€â”€ `figure7.Semantic Detection.png`  
â”‚   â””â”€â”€ `figure8.mapping.png`  
â”‚  
â”œâ”€â”€ ğŸ“œ `LICENSE` â€” Academic research-only license  
â”œâ”€â”€ ğŸª¶ `README.md` â€” Project documentation  
â””â”€â”€ ğŸ§¾ `requirements.txt` â€” Dependencies and environment setup  

---

## âš ï¸ Usage and Permissions

All **codes, figures, and datasets** in this repository were developed and curated **solely for academic research purposes** as part of  
*â€œA CLIP-Enhanced Multimodal Arbitration Framework for Explainable Hurricane-Induced Damage Assessment from Street-View Imagery.â€*

If you wish to **reuse, reproduce, modify, or distribute** any portion of the **codebase, figures, or dataset**, please **contact the author in advance** to obtain written permission.

ğŸ“© **Contact:**  
**Yifan Yang** ([yyang295@tamu.edu](mailto:yyang295@tamu.edu))  
Department of Geography, Texas A&M University  
ğŸŒ [https://rayford295.github.io](https://rayford295.github.io)

ğŸš« Unauthorized redistribution, adaptation, or commercial use of the materials in this repository is **strictly prohibited**.


